{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"pic.png\" width=\"1000px\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms as transforms\n",
    "from sklearn.decomposition import PCA\n",
    "import torch.nn as nn\n",
    "from einops import rearrange\n",
    "from torchsummary import summary\n",
    "import torch.optim as optim\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "import cv2\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_distances(x, y):\n",
    "    \"\"\"\n",
    "    Calculate the euclidean distance between tensors x and y\n",
    "    x,y dimensions are: (b, c, h, w)\n",
    "    \"\"\"\n",
    "    assert x.shape == y.shape, \"x and y must have the same shape\"\n",
    "    assert len(x.shape) == 4, \"x and y must have 4 dimensions\"\n",
    "    return torch.sqrt((x - y).pow(2).sum(dim=(1, 2, 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test calculate_distance with tensors of shape (b, c, h, w)\n",
    "x = torch.tensor([[1, 2, 3, 4, 5, 6, 7, 8], [9, 7, 6, 5, 4, 3, 2, 1]])\n",
    "y = torch.tensor([[1, 2, 3, 4, 5, 6, 7, 8], [8, 7, 6, 5, 4, 3, 2, 1]])\n",
    "x = rearrange(x, 'b (c h w) -> b c h w', c=2, h=2, w=2)\n",
    "y = rearrange(y, 'b (c h w) -> b c h w', c=2, h=2, w=2)\n",
    "distances = calculate_distances(x, y)\n",
    "assert distances[0] == 0\n",
    "assert distances[1] == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VectorQuantizedVariationalAutoencoder(nn.Module):\n",
    "    def __init__(self, input_dim, codebook_size, encoding_dim, hidden_dim):\n",
    "        \"\"\"\n",
    "        input_dim: dimension of input image: (c, h, w)\n",
    "        codebook_size: number of codebook vectors\n",
    "        encoding_dim: encodings dimension: (embedding_size, h, w)\n",
    "        hidden_dim: hidden dimension of the network, between FC layers\n",
    "        \"\"\"\n",
    "        super(VectorQuantizedVariationalAutoencoder, self).__init__()\n",
    "\n",
    "        assert len(input_dim) == 3, \"Input dimension must be 3D\"\n",
    "        assert len(encoding_dim) == 3, \"Encoding dimension must be 3D\"\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.codebook_size = codebook_size\n",
    "        self.encoding_dim = encoding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        # Unwrap input dimension\n",
    "        self._input_c = input_dim[0]\n",
    "        self._input_h = input_dim[1]\n",
    "        self._input_w = input_dim[2]\n",
    "        self._input_dim_flat = self._input_c * self._input_h * self._input_w\n",
    "\n",
    "        # Unwrap encoding dimension\n",
    "        self._embedding_dim = encoding_dim[0]\n",
    "        self._encoding_h = encoding_dim[1]\n",
    "        self._encoding_w = encoding_dim[2]\n",
    "\n",
    "        # Calculate flat encoding dimension\n",
    "        self._encoding_dim_flat = self._embedding_dim * self._encoding_h * self._encoding_w\n",
    "\n",
    "        # Setup components\n",
    "        self.__setup_encoder()\n",
    "        self.__setup_decoder()\n",
    "        self.__setup_codebook()\n",
    "    \n",
    "    def __setup_encoder(self):\n",
    "        self.enc_fc1 = nn.Linear(self._input_dim_flat, self.hidden_dim)\n",
    "        self.enc_fc2 = nn.Linear(self.hidden_dim, self._encoding_dim_flat)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def __setup_decoder(self):\n",
    "        self.dec_fc1 = nn.Linear(self._encoding_dim_flat, self.hidden_dim)\n",
    "        self.dec_fc2 = nn.Linear(self.hidden_dim, self._input_dim_flat)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def __setup_codebook(self):\n",
    "        self.codebook = nn.Embedding(self.codebook_size, self._embedding_dim)\n",
    "        assert self.codebook.weight.requires_grad == True, \"Codebook should be learnable\"\n",
    "    \n",
    "    def encode(self, x):\n",
    "        assert x.shape[1:] == (self._input_c, self._input_h, self._input_w)\n",
    "\n",
    "        # Run the layers\n",
    "        x = rearrange(x, 'b c h w -> b (c h w)') # Flatten the input\n",
    "        x = self.relu(self.enc_fc1(x))\n",
    "        x = self.relu(self.enc_fc2(x))\n",
    "\n",
    "        return x\n",
    "    \n",
    "    def decode(self, z_q):\n",
    "        assert z_q.shape[-1] == self._encoding_dim_flat\n",
    "\n",
    "        # Run the layers\n",
    "        x = self.relu(self.dec_fc1(z_q))\n",
    "        x = self.relu(self.dec_fc2(x))\n",
    "        x_hat = self.sigmoid(x)\n",
    "\n",
    "        # Reshape the output, we now have c, h, w dimensions (spatial image) instead of flat vector\n",
    "        x_hat = rearrange(x_hat, 'b (c h w) -> b c h w', c=self._input_c, h=self._input_h, w=self._input_w) \n",
    "\n",
    "        return x_hat\n",
    "    \n",
    "    def calculate_distances(self, z):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            z: Input tensor of shape (batch_size, dim_flat)\n",
    "        Returns:\n",
    "            distances: Tensor of shape (batch_size, height, width, num_embeddings)\n",
    "        \"\"\"\n",
    "        z = rearrange(z, 'b (c h w) -> (b h w) c', c=self._embedding_dim, h=self._encoding_h, w=self._encoding_w)\n",
    "\n",
    "        # Z shape: (128, 256)\n",
    "        # Codebook shape: (512, 256)\n",
    "\n",
    "        # For each of the 128 vectors in Z, we want to calculate the distance between the vector and all 512 codebook vectors\n",
    "        # ||z - e||^2 = ||z||^2 + ||e||^2 - 2 * <z,e>\n",
    "        z_norm_squared = torch.sum(z ** 2, dim=1, keepdim=True)\n",
    "        e_norm_squared = torch.sum(self.codebook.weight ** 2, dim=1)\n",
    "        inner_products = torch.matmul(z, self.codebook.weight.t())\n",
    "\n",
    "        # Distances is of shape (batch, embed_dim)\n",
    "        distances = z_norm_squared + e_norm_squared - 2 * inner_products\n",
    "        distances = rearrange(distances, '(b h w) c -> b h w c', b=z.shape[0] // (self._encoding_h * self._encoding_w), h=self._encoding_h, w=self._encoding_w)\n",
    "        return distances\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # z_flattened = z\n",
    "        # # # Unflatten to (batch, height, width, embedding_dim)\n",
    "        # z = rearrange(z, 'b (c h w) -> b h w c', c=self._embedding_dim, h=self._encoding_h, w=self._encoding_w)\n",
    "\n",
    "        # # Calculate squared distances using matrix operations:\n",
    "        # # ||z - e||^2 = ||z||^2 + ||e||^2 - 2 * <z,e>\n",
    "        # z_norm_squared = torch.sum(z_flattened ** 2, dim=1, keepdim=True)  # Shape: (batch*height*width, 1)\n",
    "        # e_norm_squared = torch.sum(self.codebook.weight ** 2, dim=1)  # Shape: (num_embeddings,)\n",
    "\n",
    "        # # Compute inner products between z and all embeddings\n",
    "        # inner_products = torch.matmul(z_flattened, self.codebook.weight.t())  # Shape: (batch*height*width, num_embeddings)\n",
    "\n",
    "        # # Calculate final distances\n",
    "        # distances = z_norm_squared + e_norm_squared - 2 * inner_products  # Shape: (batch*height*width, num_embeddings)\n",
    "\n",
    "        # # Reshape back to match input dimensions\n",
    "        # distances = distances.reshape(b, h, w, -1)  # Shape: (batch, height, width, num_embeddings)\n",
    "\n",
    "        # return distances\n",
    "\n",
    "    def quantize(self, z):\n",
    "        \"\"\"\n",
    "        z_e: (batch_size, self._encoding_dim_flat)\n",
    "\n",
    "        Get closest (euclidean distance) codebook vector z_q given z\n",
    "        \"\"\"\n",
    "        assert z.shape[-1] == self._encoding_dim_flat\n",
    "\n",
    "        distances = self.calculate_distances(z)\n",
    "\n",
    "        # Distances is shape (batch, height, width, embed_dim)\n",
    "        # Get the index of the closest codebook vector\n",
    "        min_indices = torch.argmin(distances, dim=-1)\n",
    "\n",
    "\n",
    "        # # Reshape z so we can find all the distances in the shape (b * h * w, c) where c = self._embedding_dim\n",
    "        # z = rearrange(z, 'b (c h w) -> b h w c', c=self._embedding_dim, h=self._encoding_h, w=self._encoding_w)\n",
    "\n",
    "        # # Initialize z_q, here we will add the vectors one by one (after finding the closest one) with one-hot encoding by index and then multiply with codebook\n",
    "        # z_q = torch.zeros(z.shape).to(device)\n",
    "\n",
    "        # # For each z_e latent vector (we have batch of latent vectors here, so we traverse in python loop instead) we want to find the closest codebook vector\n",
    "        # for i in range(z.shape[0]):\n",
    "        #     # Calculate distance between z_e[batch_idx] and codebook vectors...\n",
    "        #     a = z[i] - self.codebook\n",
    "        #     assert a.shape == (self.codebook_size, self.latent_dim)\n",
    "\n",
    "        #     # Calculate norm to get distances between z_e[batch_idx] and ALL other codebook vectors (we have self.codebook_size vectors)\n",
    "        #     distances = torch.norm(a, dim=1)\n",
    "        #     assert distances.shape == (self.codebook_size,)\n",
    "            \n",
    "        #     # Get the index of the closest codebook vector\n",
    "        #     min_index = torch.argmin(distances).item()\n",
    "\n",
    "        #     # Now we have the index of the closest codebook vector, get the representation vector\n",
    "        #     z_q_i = self.codebook[min_index]\n",
    "\n",
    "        #     # Add the closest codebook vector to z_q\n",
    "        #     z_q[i] = z_q_i\n",
    "\n",
    "        # # Assert shape of z_q (batch_size, latent_dim)\n",
    "        # assert z_q.shape == z.shape\n",
    "\n",
    "        # return z_q\n",
    "\n",
    "        pass\n",
    "\n",
    "    def forward(self, x):\n",
    "        assert x.shape[1:] == (self._input_c, self._input_h, self._input_w)\n",
    "\n",
    "        # Encode\n",
    "        z = self.encode(x)\n",
    "\n",
    "        # Quantize the latent vector\n",
    "        z_q = self.quantize(z)\n",
    "\n",
    "        # Add streight through estimator\n",
    "        z_q = z + (z_q - z).detach()\n",
    "        \n",
    "        # Decode\n",
    "        x_reconstructed = self.decode(z_q)\n",
    "\n",
    "        # The output image should have the same shape as the input image\n",
    "        assert x_reconstructed.shape == x.shape\n",
    "\n",
    "        # Return x hat (and also some other stuff for loss calculation and debugging)\n",
    "        return x_reconstructed, z, z_q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([36, 37])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([[1, 2, 3, 4, 5, 6, 7, 8], [9, 7, 6, 5, 4, 3, 2, 1]])\n",
    "x = rearrange(x, 'b (c h w) -> b h w c', c=2, h=2, w=2)\n",
    "x.sum(dim=(1,2,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantization of z_e\n",
    "\n",
    "To quantisize the z_e we run `torch.norm()` which calculate length of vector.\n",
    "\n",
    "For instance if `a = torch.tensor([1, 1])` then the length of vector is `sqrt(1^2 + 1^2) = sqrt(2) = 1.4142`\n",
    "\n",
    "So we do: `torch.norm(z_e - codebook)` which means we measure distance between two vectors.\n",
    "\n",
    "Then we apply `argmin`: `torch.argmin(torch.norm(z_e - codebook))` to get the index of the closest vector in the codebook.\n",
    "\n",
    "Finally we get the quantized vector: `codebook[torch.argmin(torch.norm(z_e - codebook))]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformations\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "# Load dataset\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n",
    "test_dataset = datasets.MNIST(root='./data', train=False, transform=transform, download=True)\n",
    "\n",
    "# DataLoader\n",
    "batch_size = 64\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Training\n",
    "num_epochs = 10\n",
    "\n",
    "# Model\n",
    "input_dim = (1, 28, 28)\n",
    "codebook_size = 512\n",
    "\n",
    "embedding_dim = 256 # Dimension of each codebook vector\n",
    "encoding_dim = (embedding_dim, 8, 8)\n",
    "\n",
    "hidden_dim = 1024\n",
    "\n",
    "model = VectorQuantizedVariationalAutoencoder(input_dim, codebook_size, encoding_dim, hidden_dim).to(device)\n",
    "\n",
    "# Optimizer\n",
    "lr = 1e-3\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for -: 'Tensor' and 'Embedding'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43msummary\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m28\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m28\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Shlomi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchsummary\\torchsummary.py:72\u001b[0m, in \u001b[0;36msummary\u001b[1;34m(model, input_size, batch_size, device)\u001b[0m\n\u001b[0;32m     68\u001b[0m model\u001b[38;5;241m.\u001b[39mapply(register_hook)\n\u001b[0;32m     70\u001b[0m \u001b[38;5;66;03m# make a forward pass\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;66;03m# print(x.shape)\u001b[39;00m\n\u001b[1;32m---> 72\u001b[0m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;66;03m# remove these hooks\u001b[39;00m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m h \u001b[38;5;129;01min\u001b[39;00m hooks:\n",
      "File \u001b[1;32mc:\\Users\\Shlomi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Shlomi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[5], line 170\u001b[0m, in \u001b[0;36mVectorQuantizedVariationalAutoencoder.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    167\u001b[0m z \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencode(x)\n\u001b[0;32m    169\u001b[0m \u001b[38;5;66;03m# Quantize the latent vector\u001b[39;00m\n\u001b[1;32m--> 170\u001b[0m z_q \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquantize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mz\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    172\u001b[0m \u001b[38;5;66;03m# Add streight through estimator\u001b[39;00m\n\u001b[0;32m    173\u001b[0m z_q \u001b[38;5;241m=\u001b[39m z \u001b[38;5;241m+\u001b[39m (z_q \u001b[38;5;241m-\u001b[39m z)\u001b[38;5;241m.\u001b[39mdetach()\n",
      "Cell \u001b[1;32mIn[5], line 142\u001b[0m, in \u001b[0;36mVectorQuantizedVariationalAutoencoder.quantize\u001b[1;34m(self, z)\u001b[0m\n\u001b[0;32m    139\u001b[0m \u001b[38;5;66;03m# For each z_e latent vector (we have batch of latent vectors here, so we traverse in python loop instead) we want to find the closest codebook vector\u001b[39;00m\n\u001b[0;32m    140\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(z\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]):\n\u001b[0;32m    141\u001b[0m     \u001b[38;5;66;03m# Calculate distance between z_e[batch_idx] and codebook vectors...\u001b[39;00m\n\u001b[1;32m--> 142\u001b[0m     a \u001b[38;5;241m=\u001b[39m \u001b[43mz\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcodebook\u001b[49m\n\u001b[0;32m    143\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m a\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m==\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcodebook_size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlatent_dim)\n\u001b[0;32m    145\u001b[0m     \u001b[38;5;66;03m# Calculate norm to get distances between z_e[batch_idx] and ALL other codebook vectors (we have self.codebook_size vectors)\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: unsupported operand type(s) for -: 'Tensor' and 'Embedding'"
     ]
    }
   ],
   "source": [
    "summary(model, (1, 28, 28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta = 0.25 # Scale factor for commitment loss\n",
    "\n",
    "def loss_function(x, x_hat, z_e, z_q):\n",
    "    \"\"\"\n",
    "    z_e: latent vector before quantization\n",
    "    z_q: latent vector after quantization\n",
    "    \"\"\"\n",
    "    # Reconstruction loss\n",
    "    recon_loss = nn.BCELoss(reduction='sum')(x_hat, x)\n",
    "\n",
    "    # Quantization loss\n",
    "    quant_loss = nn.functional.mse_loss(z_q, z_e.detach())\n",
    "\n",
    "    # Commitment loss\n",
    "    commit_loss = nn.functional.mse_loss(z_q.detach(), z_e)\n",
    "\n",
    "    # Total loss\n",
    "    loss = recon_loss + quant_loss + beta * commit_loss\n",
    "\n",
    "    return loss, recon_loss, quant_loss, commit_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_step(images, loss_history, recon_loss_history, quant_loss_history, commit_loss_history):\n",
    "    # Forward pass\n",
    "    x_hat, z_e, z_q = model(images)\n",
    "\n",
    "    # Calculate loss\n",
    "    loss, recon_loss, quant_loss, commit_loss = loss_function(images, x_hat, z_e, z_q)\n",
    "\n",
    "    # Add all three losses to history\n",
    "    loss_history.append(loss.item())\n",
    "    recon_loss_history.append(recon_loss.item())\n",
    "    quant_loss_history.append(quant_loss.item())\n",
    "    commit_loss_history.append(commit_loss.item())\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(num_epochs):\n",
    "    loss_history = []\n",
    "    quant_loss_history = []\n",
    "    recon_loss_history = []\n",
    "    commit_loss_history = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "\n",
    "        overall_loss = 0\n",
    "\n",
    "        # Use tqdm for progress tracking\n",
    "        with tqdm(train_loader, desc=f\"Epoch [{epoch+1}/{num_epochs}]\") as t:\n",
    "            for images, _ in t:\n",
    "                images = images.to(device)\n",
    "\n",
    "                # Zero gradients before performing a backward pass\n",
    "                optimizer.zero_grad() \n",
    "\n",
    "                # Perform a forward pass and calculate loss\n",
    "                loss = training_step(images, loss_history, recon_loss_history, quant_loss_history, commit_loss_history)\n",
    "\n",
    "                # Add loss to overall loss\n",
    "                overall_loss += loss.item()\n",
    "\n",
    "                # Propagate the loss backward\n",
    "                loss.backward() # Compute gradients (of KL + recon losses)\n",
    "                optimizer.step() # Adjust weights of the model\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Recon Loss: {np.mean(recon_loss_history).item():.4f}, KL Loss: {np.mean(quant_loss_history).item():.4f}\")\n",
    "        print(\"Average loss: \", overall_loss / len(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 5\n",
    "train(num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "torch.save(model, \"without_ste.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot 8 images (original image) and to their right plot the reconstructed image\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    images, _ = next(iter(test_loader))\n",
    "    images = images[:8].to(device)\n",
    "\n",
    "    outputs, _, _, _, _ = model(images)\n",
    "\n",
    "    images = images.cpu()\n",
    "    outputs = outputs.cpu()\n",
    "\n",
    "    print(outputs.shape)\n",
    "\n",
    "    fig, axes = plt.subplots(2, 8, figsize=(20, 5))\n",
    "    for i in range(8):\n",
    "        axes[0, i].imshow(images[i].view(28, 28), cmap='gray')\n",
    "        axes[0, i].axis('off')\n",
    "        axes[1, i].imshow(outputs[i].view(28, 28), cmap='gray')\n",
    "        axes[1, i].axis('off')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_e.requires_grad"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
