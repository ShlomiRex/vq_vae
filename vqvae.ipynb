{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"pic.png\" width=\"1000px\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms as transforms\n",
    "from sklearn.decomposition import PCA\n",
    "import torch.nn as nn\n",
    "from einops import rearrange\n",
    "from torchsummary import summary\n",
    "import torch.optim as optim\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "import cv2\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_distances(x, y):\n",
    "    \"\"\"\n",
    "    Calculate the euclidean distance between tensors x and y\n",
    "    x,y dimensions are: (b, c, h, w)\n",
    "    \"\"\"\n",
    "    assert x.shape == y.shape, \"x and y must have the same shape\"\n",
    "    assert len(x.shape) == 4, \"x and y must have 4 dimensions\"\n",
    "    return torch.sqrt((x - y).pow(2).sum(dim=(1, 2, 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test calculate_distance with tensors of shape (b, c, h, w)\n",
    "x = torch.tensor([[1, 2, 3, 4, 5, 6, 7, 8], [9, 7, 6, 5, 4, 3, 2, 1]])\n",
    "y = torch.tensor([[1, 2, 3, 4, 5, 6, 7, 8], [8, 7, 6, 5, 4, 3, 2, 1]])\n",
    "x = rearrange(x, 'b (c h w) -> b c h w', c=2, h=2, w=2)\n",
    "y = rearrange(y, 'b (c h w) -> b c h w', c=2, h=2, w=2)\n",
    "distances = calculate_distances(x, y)\n",
    "assert distances[0] == 0\n",
    "assert distances[1] == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VectorQuantizedVariationalAutoencoder(nn.Module):\n",
    "    def __init__(self, input_dim, codebook_size, encoding_dim, hidden_dim):\n",
    "        \"\"\"\n",
    "        input_dim: dimension of input image: (c, h, w)\n",
    "        codebook_size: number of codebook vectors\n",
    "        encoding_dim: encodings dimension: (embedding_size, h, w)\n",
    "        hidden_dim: hidden dimension of the network, between FC layers\n",
    "        \"\"\"\n",
    "        super(VectorQuantizedVariationalAutoencoder, self).__init__()\n",
    "\n",
    "        assert len(input_dim) == 3, \"Input dimension must be 3D\"\n",
    "        assert len(encoding_dim) == 3, \"Encoding dimension must be 3D\"\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.codebook_size = codebook_size\n",
    "        self.encoding_dim = encoding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        # Unwrap input dimension\n",
    "        self._input_c = input_dim[0]\n",
    "        self._input_h = input_dim[1]\n",
    "        self._input_w = input_dim[2]\n",
    "        self._input_dim_flat = self._input_c * self._input_h * self._input_w\n",
    "\n",
    "        # Unwrap encoding dimension\n",
    "        self._embedding_dim = encoding_dim[0]\n",
    "        self._encoding_h = encoding_dim[1]\n",
    "        self._encoding_w = encoding_dim[2]\n",
    "\n",
    "        # Calculate flat encoding dimension\n",
    "        self._encoding_dim_flat = self._embedding_dim * self._encoding_h * self._encoding_w\n",
    "\n",
    "        # Setup components\n",
    "        self.__setup_encoder()\n",
    "        self.__setup_decoder()\n",
    "        self.__setup_codebook()\n",
    "    \n",
    "    def __setup_encoder(self):\n",
    "        self.enc_fc1 = nn.Linear(self._input_dim_flat, self.hidden_dim)\n",
    "        self.enc_fc2 = nn.Linear(self.hidden_dim, self._encoding_dim_flat)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def __setup_decoder(self):\n",
    "        self.dec_fc1 = nn.Linear(self._encoding_dim_flat, self.hidden_dim)\n",
    "        self.dec_fc2 = nn.Linear(self.hidden_dim, self._input_dim_flat)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def __setup_codebook(self):\n",
    "        self.codebook = nn.Embedding(self.codebook_size, self._embedding_dim)\n",
    "        assert self.codebook.weight.requires_grad == True, \"Codebook should be learnable\"\n",
    "    \n",
    "    def encode(self, x):\n",
    "        assert x.shape[1:] == (self._input_c, self._input_h, self._input_w)\n",
    "\n",
    "        # Run the layers\n",
    "        x = rearrange(x, 'b c h w -> b (c h w)') # Flatten the input\n",
    "        x = self.relu(self.enc_fc1(x))\n",
    "        x = self.relu(self.enc_fc2(x))\n",
    "\n",
    "        # Unflatten, last dimension is embedding dim\n",
    "        x = rearrange(x, 'b (c h w) -> b h w c', c=self._embedding_dim, h=self._encoding_h, w=self._encoding_w)\n",
    "\n",
    "        return x\n",
    "    \n",
    "    def decode(self, z_q):\n",
    "        assert z_q.shape[-1] == self._encoding_dim_flat\n",
    "\n",
    "        # Run the layers\n",
    "        x = self.relu(self.dec_fc1(z_q))\n",
    "        x = self.relu(self.dec_fc2(x))\n",
    "        x_hat = self.sigmoid(x)\n",
    "\n",
    "        # Reshape the output, we now have c, h, w dimensions (spatial image) instead of flat vector\n",
    "        x_hat = rearrange(x_hat, 'b (c h w) -> b c h w', c=self._input_c, h=self._input_h, w=self._input_w) \n",
    "\n",
    "        return x_hat\n",
    "    \n",
    "    def calculate_distances(self, z):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            z: Input tensor of shape (batch_size, dim_flat)\n",
    "        Returns:\n",
    "            distances: Tensor of shape (batch_size, height, width, num_embeddings)\n",
    "        \"\"\"\n",
    "        z = rearrange(z, 'b (c h w) -> (b h w) c', c=self._embedding_dim, h=self._encoding_h, w=self._encoding_w)\n",
    "\n",
    "        # Z shape: (128, 256)\n",
    "        # Codebook shape: (512, 256)\n",
    "\n",
    "        # For each of the 128 vectors in Z, we want to calculate the distance between the vector and all 512 codebook vectors\n",
    "        # ||z - e||^2 = ||z||^2 + ||e||^2 - 2 * <z,e>\n",
    "        z_norm_squared = torch.sum(z ** 2, dim=1, keepdim=True)\n",
    "        e_norm_squared = torch.sum(self.codebook.weight ** 2, dim=1)\n",
    "        inner_products = torch.matmul(z, self.codebook.weight.t())\n",
    "\n",
    "        # Distances is of shape (batch, embed_dim)\n",
    "        distances = z_norm_squared + e_norm_squared - 2 * inner_products\n",
    "        distances = rearrange(distances, '(b h w) c -> b h w c', b=z.shape[0] // (self._encoding_h * self._encoding_w), h=self._encoding_h, w=self._encoding_w)\n",
    "        return distances\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # z_flattened = z\n",
    "        # # # Unflatten to (batch, height, width, embedding_dim)\n",
    "        # z = rearrange(z, 'b (c h w) -> b h w c', c=self._embedding_dim, h=self._encoding_h, w=self._encoding_w)\n",
    "\n",
    "        # # Calculate squared distances using matrix operations:\n",
    "        # # ||z - e||^2 = ||z||^2 + ||e||^2 - 2 * <z,e>\n",
    "        # z_norm_squared = torch.sum(z_flattened ** 2, dim=1, keepdim=True)  # Shape: (batch*height*width, 1)\n",
    "        # e_norm_squared = torch.sum(self.codebook.weight ** 2, dim=1)  # Shape: (num_embeddings,)\n",
    "\n",
    "        # # Compute inner products between z and all embeddings\n",
    "        # inner_products = torch.matmul(z_flattened, self.codebook.weight.t())  # Shape: (batch*height*width, num_embeddings)\n",
    "\n",
    "        # # Calculate final distances\n",
    "        # distances = z_norm_squared + e_norm_squared - 2 * inner_products  # Shape: (batch*height*width, num_embeddings)\n",
    "\n",
    "        # # Reshape back to match input dimensions\n",
    "        # distances = distances.reshape(b, h, w, -1)  # Shape: (batch, height, width, num_embeddings)\n",
    "\n",
    "        # return distances\n",
    "\n",
    "    def quantize(self, z):\n",
    "        \"\"\"\n",
    "        z_e: (batch_size, encoding_height, encoding_width, embedding_dim)\n",
    "\n",
    "        Get closest (euclidean distance) codebook vector z_q given z\n",
    "        \"\"\"\n",
    "        assert z.shape[1:] == (self._encoding_h, self._encoding_w, self._embedding_dim)\n",
    "\n",
    "        flat_input = rearrange(z, 'b h w c -> (b h w) c')\n",
    "        distances2 = (torch.sum(flat_input**2, dim=1, keepdim=True)\n",
    "            + torch.sum(self._embedding.weight**2, dim=1)\n",
    "            - 2 * torch.matmul(flat_input, self._embedding.weight.t()))\n",
    "        \n",
    "        distances = self.calculate_distances(z)\n",
    "\n",
    "        # Distances is shape (batch, height, width, embed_dim)\n",
    "        # Get the index of the closest codebook vector\n",
    "        min_indices = torch.argmin(distances, dim=-1)\n",
    "\n",
    "\n",
    "        # # Reshape z so we can find all the distances in the shape (b * h * w, c) where c = self._embedding_dim\n",
    "        # z = rearrange(z, 'b (c h w) -> b h w c', c=self._embedding_dim, h=self._encoding_h, w=self._encoding_w)\n",
    "\n",
    "        # # Initialize z_q, here we will add the vectors one by one (after finding the closest one) with one-hot encoding by index and then multiply with codebook\n",
    "        # z_q = torch.zeros(z.shape).to(device)\n",
    "\n",
    "        # # For each z_e latent vector (we have batch of latent vectors here, so we traverse in python loop instead) we want to find the closest codebook vector\n",
    "        # for i in range(z.shape[0]):\n",
    "        #     # Calculate distance between z_e[batch_idx] and codebook vectors...\n",
    "        #     a = z[i] - self.codebook\n",
    "        #     assert a.shape == (self.codebook_size, self.latent_dim)\n",
    "\n",
    "        #     # Calculate norm to get distances between z_e[batch_idx] and ALL other codebook vectors (we have self.codebook_size vectors)\n",
    "        #     distances = torch.norm(a, dim=1)\n",
    "        #     assert distances.shape == (self.codebook_size,)\n",
    "            \n",
    "        #     # Get the index of the closest codebook vector\n",
    "        #     min_index = torch.argmin(distances).item()\n",
    "\n",
    "        #     # Now we have the index of the closest codebook vector, get the representation vector\n",
    "        #     z_q_i = self.codebook[min_index]\n",
    "\n",
    "        #     # Add the closest codebook vector to z_q\n",
    "        #     z_q[i] = z_q_i\n",
    "\n",
    "        # # Assert shape of z_q (batch_size, latent_dim)\n",
    "        # assert z_q.shape == z.shape\n",
    "\n",
    "        # return z_q\n",
    "\n",
    "        pass\n",
    "\n",
    "    def forward(self, x):\n",
    "        assert x.shape[1:] == (self._input_c, self._input_h, self._input_w)\n",
    "\n",
    "        # Encode\n",
    "        z = self.encode(x)\n",
    "\n",
    "        # Quantize the latent vector\n",
    "        z_q = self.quantize(z)\n",
    "\n",
    "        # Add streight through estimator\n",
    "        z_q = z + (z_q - z).detach()\n",
    "        \n",
    "        # Decode\n",
    "        x_reconstructed = self.decode(z_q)\n",
    "\n",
    "        # The output image should have the same shape as the input image\n",
    "        assert x_reconstructed.shape == x.shape\n",
    "\n",
    "        # Return x hat (and also some other stuff for loss calculation and debugging)\n",
    "        return x_reconstructed, z, z_q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([[1, 2, 3, 4, 5, 6, 7, 8], [9, 7, 6, 5, 4, 3, 2, 1]])\n",
    "x = rearrange(x, 'b (c h w) -> b h w c', c=2, h=2, w=2)\n",
    "x.sum(dim=(1,2,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantization of z_e\n",
    "\n",
    "To quantisize the z_e we run `torch.norm()` which calculate length of vector.\n",
    "\n",
    "For instance if `a = torch.tensor([1, 1])` then the length of vector is `sqrt(1^2 + 1^2) = sqrt(2) = 1.4142`\n",
    "\n",
    "So we do: `torch.norm(z_e - codebook)` which means we measure distance between two vectors.\n",
    "\n",
    "Then we apply `argmin`: `torch.argmin(torch.norm(z_e - codebook))` to get the index of the closest vector in the codebook.\n",
    "\n",
    "Finally we get the quantized vector: `codebook[torch.argmin(torch.norm(z_e - codebook))]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformations\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "# Load dataset\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n",
    "test_dataset = datasets.MNIST(root='./data', train=False, transform=transform, download=True)\n",
    "\n",
    "# DataLoader\n",
    "batch_size = 64\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Training\n",
    "num_epochs = 10\n",
    "\n",
    "# Model\n",
    "input_dim = (1, 28, 28)\n",
    "codebook_size = 512\n",
    "\n",
    "embedding_dim = 64 # Dimension of each codebook vector\n",
    "encoding_dim = (embedding_dim, 8, 8)\n",
    "\n",
    "hidden_dim = 1024\n",
    "\n",
    "model = VectorQuantizedVariationalAutoencoder(input_dim, codebook_size, encoding_dim, hidden_dim).to(device)\n",
    "\n",
    "# Optimizer\n",
    "lr = 1e-3\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Test distances\n",
    "# codebook_size = 2\n",
    "# encoding_dim = (2, 1, 1)\n",
    "# hidden_dim = 1024\n",
    "# model = VectorQuantizedVariationalAutoencoder(input_dim, codebook_size, encoding_dim, hidden_dim).to(device)\n",
    "\n",
    "# # Set embeddings shape (1, 2)\n",
    "# model.codebook.weight.data = torch.tensor([[1, 2]]).float()\n",
    "# print(model.codebook.weight.shape)\n",
    "\n",
    "# # b = 1, c = 1, h = 1, w = 2\n",
    "# x = torch.tensor([\n",
    "#     [\n",
    "#         [\n",
    "#             [1, 2]\n",
    "#         ]\n",
    "#     ]]) \n",
    "# x = rearrange(x, '1 1 1 w -> 1 w')\n",
    "# print(x.shape)\n",
    "# distances = model.calculate_distances(x)\n",
    "\n",
    "# # Assert distance\n",
    "# assert distances[0, 0, 0, 0] == 0\n",
    "# assert distances[0, 0, 0, 1] == 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(model, (1, 28, 28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta = 0.25 # Scale factor for commitment loss\n",
    "\n",
    "def loss_function(x, x_hat, z_e, z_q):\n",
    "    \"\"\"\n",
    "    z_e: latent vector before quantization\n",
    "    z_q: latent vector after quantization\n",
    "    \"\"\"\n",
    "    # Reconstruction loss\n",
    "    recon_loss = nn.BCELoss(reduction='sum')(x_hat, x)\n",
    "\n",
    "    # Quantization loss\n",
    "    quant_loss = nn.functional.mse_loss(z_q, z_e.detach())\n",
    "\n",
    "    # Commitment loss\n",
    "    commit_loss = nn.functional.mse_loss(z_q.detach(), z_e)\n",
    "\n",
    "    # Total loss\n",
    "    loss = recon_loss + quant_loss + beta * commit_loss\n",
    "\n",
    "    return loss, recon_loss, quant_loss, commit_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_step(images, loss_history, recon_loss_history, quant_loss_history, commit_loss_history):\n",
    "    # Forward pass\n",
    "    x_hat, z_e, z_q = model(images)\n",
    "\n",
    "    # Calculate loss\n",
    "    loss, recon_loss, quant_loss, commit_loss = loss_function(images, x_hat, z_e, z_q)\n",
    "\n",
    "    # Add all three losses to history\n",
    "    loss_history.append(loss.item())\n",
    "    recon_loss_history.append(recon_loss.item())\n",
    "    quant_loss_history.append(quant_loss.item())\n",
    "    commit_loss_history.append(commit_loss.item())\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(num_epochs):\n",
    "    loss_history = []\n",
    "    quant_loss_history = []\n",
    "    recon_loss_history = []\n",
    "    commit_loss_history = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "\n",
    "        overall_loss = 0\n",
    "\n",
    "        # Use tqdm for progress tracking\n",
    "        with tqdm(train_loader, desc=f\"Epoch [{epoch+1}/{num_epochs}]\") as t:\n",
    "            for images, _ in t:\n",
    "                images = images.to(device)\n",
    "\n",
    "                # Zero gradients before performing a backward pass\n",
    "                optimizer.zero_grad() \n",
    "\n",
    "                # Perform a forward pass and calculate loss\n",
    "                loss = training_step(images, loss_history, recon_loss_history, quant_loss_history, commit_loss_history)\n",
    "\n",
    "                # Add loss to overall loss\n",
    "                overall_loss += loss.item()\n",
    "\n",
    "                # Propagate the loss backward\n",
    "                loss.backward() # Compute gradients (of KL + recon losses)\n",
    "                optimizer.step() # Adjust weights of the model\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Recon Loss: {np.mean(recon_loss_history).item():.4f}, KL Loss: {np.mean(quant_loss_history).item():.4f}\")\n",
    "        print(\"Average loss: \", overall_loss / len(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 5\n",
    "train(num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "torch.save(model, \"without_ste.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot 8 images (original image) and to their right plot the reconstructed image\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    images, _ = next(iter(test_loader))\n",
    "    images = images[:8].to(device)\n",
    "\n",
    "    outputs, _, _, _, _ = model(images)\n",
    "\n",
    "    images = images.cpu()\n",
    "    outputs = outputs.cpu()\n",
    "\n",
    "    print(outputs.shape)\n",
    "\n",
    "    fig, axes = plt.subplots(2, 8, figsize=(20, 5))\n",
    "    for i in range(8):\n",
    "        axes[0, i].imshow(images[i].view(28, 28), cmap='gray')\n",
    "        axes[0, i].axis('off')\n",
    "        axes[1, i].imshow(outputs[i].view(28, 28), cmap='gray')\n",
    "        axes[1, i].axis('off')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_e.requires_grad"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
